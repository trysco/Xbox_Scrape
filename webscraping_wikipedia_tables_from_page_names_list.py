# -*- coding: utf-8 -*-
"""Webscraping_Wikipedia_Tables_from_Page_Names_List.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rv_OXTSs7irmDar4vzEu5zptZ0tzHrkl

##Download packages
"""

#Import libraries
!pip install wikipedia
import wikipedia
import urllib
from bs4 import BeautifulSoup as bsoup
import bleach
import pandas as pd
import requests
from bs4 import BeautifulSoup

#Import Colab Data Table Display 
from google.colab import data_table
data_table.enable_dataframe_formatter()

"""##Import names of all pages to be scraped (CSV file) and search wikipedia for each name"""

#Import csv with list of url links
from google.colab import files
uploaded = files.upload()

#Make search terms into dataframe
import pandas as pd
df = pd.read_csv('GameCube_Companies - Sheet1 (2).csv')  

link_list = df['All_Companies'].to_list()
link_list

#Search wikipedia for each term and append first result to a new list
search_results = []
for item in link_list:   
  result = wikipedia.search(item, results = 1)
  search_results.append(result)

#Check search results
len(search_results)

#Iterate through list of search results, get url of each page and append to list
page_urls = []
#Store list of companies where no URL was found for first search result
no_urls = []

for item in search_results:
    try:
      url = wikipedia.page(item, auto_suggest=False).url
      page_urls.append(url)
    except:
      no_urls.append(item)

page_urls

#Get # of page urls that were found
print(len(page_urls))

#Get # of pages that were not found
print(len(no_urls))

no_urls

"""##Use BeautifulSoup to scrape pages from list of retrieved urls"""

#Create empty list for soup objects and titles
soups = []
all_scraped_title_list = []
all_scraped_title_strings = []

# create function to harvest data from each URL
def get_data(page_url):
      page = requests.get(page_url)
      soup = BeautifulSoup(page.text, 'html.parser')
      #Get title of page
      title = soup.find(id="firstHeading")
      all_scraped_title_list.append(title)
      #Append scraped data to list
      soups.append(soup)
  

# call the function for each URL in the list
for url in page_urls:
    get_data(url) 

#Get strings for all pages with titles
for title in all_scraped_title_list: 
    string = title.string
    #Turn into string
    string = str(string)
    all_scraped_title_strings.append(string)

#Confirm # of pages scraped (should be same as page_urls)
len(all_scraped_title_strings)

"""##Confirm how many pages have the table we're looking for (infobox vcard)"""

titles_with_tables = []
all_columns = []

#Find table with headquarters values (use inspect to find on wiki pages)
for soup in soups: 
  title = soup.find(id="firstHeading")
  tables = soup.find_all('table',class_='infobox vcard')
  #Get names of all columns in table and append to list
  for table in tables:
    column_names = [item.get_text() for item in tables[0].find_all('th')]
    all_columns.append(column_names)
    #Get names of all titles with the right table and append to list
    titles_with_tables.append(title)

#Get list of strings for all the pages with the correct table
table_title_strings = []
for title in titles_with_tables: 
    string = title.string
    #Turn into string
    string = str(string)
    table_title_strings.append(string)
len(table_title_strings)

#Get all titles where right table was NOT found and append to list for later
from collections import Counter
no_table_strings = list((Counter(all_scraped_title_strings)-Counter(table_title_strings)).elements())

"""##Confirm how many pages have the value we're looking for (headquarters)"""

#Associate all titles where table was found with table column values
keys = table_title_strings
values = all_columns
res = {keys[i]: values[i] for i in range(len(keys))}

#Create dataframe based on title and table column values
test = pd.DataFrame.from_dict(res, orient='index')

#Iterate through dataframe and all company rows that include headquarters as column value 
df = test[test.apply(lambda x: x.str.contains('Headquarters')).any(axis=1)]
df = df.reset_index()
df_companies = df['index']

#Append matching company names to list
titles_with_hq = df_companies.tolist()

# Get all titles where headquarters was NOT included as row in table and append to list for later
from collections import Counter
no_hq_strings = list((Counter(table_title_strings)-Counter(titles_with_hq)).elements())

len(titles_with_hq)

len(no_hq_strings)

"""#Webscrape only URLS of pages where table and HQ column are present"""

#Search wikipedia for each term and append first result to a new list
search_results2 = []
for item in titles_with_hq:
  result = wikipedia.search(item, results = 1)
  search_results2.append(result)

#Check search results
search_results2

#Iterate through list of search results, get url of each page and append to list
page_urls2 = []
no_urls2 = []

for item in search_results2:
    try:
      url = wikipedia.page(item, auto_suggest=False).url
      page_urls2.append(url)
    except:
      no_urls2.append(item)

page_urls2

#Get # of page urls that were found
print(len(page_urls2))

#Get # of pages that were not found (should be 0)
print(len(no_urls2))

#Use list of urls established above
#Set up lists to append scraped data
soups2 = []
all_scraped_title_list2 = []

# create function to harvest data from each URL
def get_data2(page_url):
      page = requests.get(page_url)
      soup = BeautifulSoup(page.text, 'html.parser')
      #Get title of page
      title = soup.find(id="firstHeading")
      all_scraped_title_list2.append(title)
      #Append scraped data to list
      soups2.append(soup)
  

 # call the function for each URL in the list
for url in page_urls2:
    get_data2(url)

#Get correct table for all titles and append table to list, title to list
titles_with_tables2 = []
right_tables = []
for soup in soups2: 
  title = soup.find(id="firstHeading")
  tables = soup.find_all('table',class_='infobox vcard')
  for table in tables:
    titles_with_tables2.append(title)
    right_tables.append(table)
  
#Append all titles from pages with scraped tables to list
title_strings2 = []
for title in titles_with_tables2: 
    string = title.string
    #Turn into string
    string = str(string)
    title_strings2.append(string)
title_strings2

"""## Create dataframe from tables of all pages and clean for HQ values only"""

#Append values from each correct table to dataframe
scraped_tables_df = pd.DataFrame()

#Convert table into dataframe
for table in right_tables:
  scraped_tables_df = scraped_tables_df.append(pd.read_html(str(table)))

# Clean datafarme (transpose, add column headers)
scraped_tables_df = scraped_tables_df.T
scraped_tables_df.columns = scraped_tables_df.iloc[0]
scraped_tables_df = scraped_tables_df.drop(scraped_tables_df.index[0])
scraped_tables_df

#Keep only headquarters values
scraped_tables_df = scraped_tables_df['Headquarters']

#Transpose
scraped_tables_df = scraped_tables_df.T
scraped_tables_df

"""## Associate HQ columns with company names, conduct final cleaning, and download to CSV"""

#Add company titles as new column
scraped_tables_df['Company'] = title_strings2
scraped_tables_df

#Clean dataframe
scraped_tables_df = scraped_tables_df.reset_index(drop=True)
cols = scraped_tables_df.columns.tolist()
cols = cols[-1:] + cols[:-1]
scraped_tables_df = scraped_tables_df[cols]
scraped_tables_df.rename(columns={1: 'Headquarters'}, inplace=True)


#Print finalized dataframe
scraped_tables_df

#Export to csv
from google.colab import files
scraped_tables_df.to_csv('headquarters_output.csv', encoding = 'utf-8-sig') 
files.download('headquarters_output.csv')

"""#Compile companies without output for further investigation"""

#List of companies where no page URL was found 
#Possible fix - first URL was ambiguous, rerun with different # search result
no_urls

#List of companies where no table was found
#Possible fix - searching wrong URL, rerun with different # search result
no_table_strings

#List of companies where headquarters was not in tables
#Possible fix - look @ tables by hand to determine if correct page; if so, enter HQ manually/from different site
no_hq_strings


#Append all into dataframe and clean
no_results = [no_urls, no_table_strings, no_hq_strings]
no_results_df=pd.DataFrame(no_results)
no_results = no_results_df.T
no_results = no_results.rename(columns={0: "First_Search_Result_Not_Scraped", 1: "No_Infobox_VCard_Table", 2: "No_HQ_Column"}, errors="raise")

#Download results
no_results.to_csv('no_results.csv', encoding = 'utf-8-sig') 
files.download('no_results.csv')

#Possible fixes for pages with wrong URLs
#Re-search and append (if correct page is not first result)
wikipedia.search("Zed Two", results = 4)
search_results.append("result")